p(ci|wi) = p(wi|ci) * p(ci) / p(wi)

p(wi|ci) = p(w0|c0) * p(w1|c0) * p(w2|c0). p(w0|c1) * p(w0|c1) * p(w0|c1)
#dont need calculate p(wi) = p(w0) * p(w1) * p(w2)
伪代码：
计算每个类别的文档数量
对每篇训练文档：
   对每个类别：
      如果词条出现在文档中 --> 增加该词条的计数+1
	  增加所有词条的计数 + 1
   对每个类别：
      对每个词条：
	     将该词条的数目除以总词条数目得到条件概率
   返回每个类别的条件概率

tips:
1. 取出文档中最经常出现的一些词语
2. log可以避免下溢出或者浮点数舍入导致的错误
3. 训练集 和 测试集  交叉验证

######
example:
1. 邮件过滤系统
   判断每封邮件的正面或者负面，然后过滤
2. 观察两个地区人们的用词习惯
   1. 本案例只关注词语的条件概率 ， 不关注最终的性质
   2. 本案例中测试了保留最常用词和不保留的错误率，发现不保留的错误率更低
   当然案例中不关注分类结果，所以影响不大
